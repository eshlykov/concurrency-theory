## Транзакции

Рассмотрим простую модель. У нас есть некоторый _DataStore_ с двумя операциями:
* _Read(k)_ — прочитать по ключу значение;
* _Write(k, v)_ — записать по ключу значение.

**Транзакция** - это маленькая программа, которая начинается со специальной команды `CommitTx` и заканчивается одной из двух команд — `CommitTx` или `AbortTx`. А между этими командами у нас есть чтения и записи плюс какая-то логика, которая управляет этими чтениями и записями.

Транзакция может завершится успешно или неуспешно. `Commit` означает, что все записи, сделанные внутри транзакции, мы хотим применить к хранилищу. Команда `Abort` означает, что сделанные изменения надо откатить.

Когда надо добровольно делать `Abort`? Например, мы хотим перевести деньги с одного счета на другой, но выясняется, что суммы недостаточно. Вообще, транзакция не обязана завершатся успешно, даже если мы сделали `Commit`. Это слуается сплошь и рядом, так как транзакции могут конкурировать друг с другом.

Таким образом, транзакция может отмениться не по причине нарушения каких-то инвариантов, которые мы контролируем, а из-за каких-то деталей внутренней реализации.

### ACID — требования к транзакциям

* A (_atomicity_) — если транзакция отменилась или транзакция выполнялась, а машина рестартовала, то в вашей базе данных от транзакции никаких следов не останется (то есть транзакция применяется либо целиком, либо не применяется вообще);
* C (_consistency_) — это требования, что база данных целиком находится в некотором глобальном согласованном состоянии, то есть в ней выполняются какие-то инварианты (с другой стороны, база данных о них ничего не знает, это инвариант пользователя);
* I (_isolation_) — как ведут себя конкурирующие транзакции, как их выполнение сказывается на базе данных;
* D (_durability_) — после коммита транзакции данные остаются в базе данных навсегда, даже если машины перезапускаются и прочее.

### Scheduler

За изоляцию в базе отвечает специальный компонент, который мы назовем **планировщиком** (scheduler).

Под планировщиком находится хранилище. К планировщику с разных сторон поступаются операции из транзакции (чтения, записи, `StartTx`, `CommitTx`, `AbortTx`).

Задача планировщика брать эти обращения из разных транзакций и перенаправлять их в хранилище, причем требуется, чтобы транзакции выполнялись как будто бы атомарно.

Само хранилище будем считать линеаризуемым.

Планировщик порождает **расписание** (schedule) — последовательность чтений и записей из разных транзакций. Например: _Write(1, x); Write(2, y); Read(1, y)_.

Расписание работает параллельно, но из-за линеаризуемости хранилища можно считать, что получаемые расписания последовательные.

### View-Serializability

Расписание называется **последовательным** (serial), если операции всех транзакций сгруппированы по номеру транзакции.

Расписание называется **view-сериализуемым** (view-serializable), если существует такое последовательное расписание (перестановка операций из исходного расписания), что все чтения возвращают одно и то же.

Нам совершенно неважно, как планировщик перемешал эти операции. Более того, мы это даже не наблюдаем. Но нам важно, что все чтения из хранилища объясняются некоторым последовательным выполнением этих траназакций.

Однако view-сериализуемые расписания могут быть устроены произвольным образом. Например, пусть есть три транзакции:

| _T1_          | _T2_          | _T3_          |
| ------------- | ------------- | ------------- |
| _Write(x, 1)_ |               |               |
|               | _Write(x, 2)_ |               |
|               | _Write(y, 2)_ |               |
| _Write(y, 1)_ |               |               |
|               |               | _Write(y, 3)_ |

До прихода последней транзакции расписания не было view-сериализуемости, но внезапно пришедшая транзакция _T3_ это замаскировала.

Наш планировщик не может предвидеть будущее. Нам нужен какой-то простой детерменированный онлайн-алгоритм, который будет генерировать только хорошие расписания. На приведенный пример он способен не будет, так как не знает, что придет новая транзакция и спасет его.

Вообще, тестирование расписание на сериализуемость — NP-полная задача. Как они устроены, непонятно.

### Conflict-Serializability

Рассмотрим все view-сериализуемые расписания и выделим в нем подкласс, к которому существует простой критерий принадлежности.

Назовем два обращения их двух разных транзакций **конфликтующими**, если они обращаются по одному ключу и хотя бы одно из этих обращений — запись.

Если у нас есть расписание, в котором две операции конфликтуют, то их порядок зафиксирован, мы их не можем поменять местами. А если две операции не конфликтуют, то их можно свопнуть, и никто ничего не заметит, так как на чтение это не повлияет.

Два расписания **конфликтно-эквивалентны** (conflict-equivalent), если одно из них можно перевести в другое серией свопов неконфликтующих операций.

Наконец, расписание называется **конфликтно-сериализуемым** (conflict-serializable), если существует конфликтно-эквивалентное ему последовательное расписание.

Очевидно, конфликтно-сериализуемые расписания являются view-сериализуемыми, поскольку мы явно приводим их к последовательному виду.

Расписисание из примера сериализуемым не является.

Зачем вообще нужна сериализуемость? Пусть наша база гарантирует нам только сериализумеые исполения. Пусть мы запускаем какие-то конкурирующие транзакции. Если мы запустим транзакции поодиночке и они будут переводить базу из корректного состояния в корректное, то мы получаем гарантирю, что несмотря на конкуренция, база всегда будет в корректном состоянии.

### Conflict Graph

Окзывается, мы можем легко проверять, является ли расписание конфликтно-сериализуемым.

Построим **граф конфликтов** (conflict graph, CG) для предложенного расписания. Вершины — это транзакции. Между транзакциями проводится дуга от _T1 к _T2_, если в данном расписании есть два конфликтующих обращения _O1_ и _O2_ из транзакций _T1_ и _T2_ соответственно, причем _O1_ предшествует _O2_.

Если такая пара есть, то их нельзя свопнуть, а значит, транзакция _T1_ должна предшествовать транзакции _T2_.

Критерий конфликтной сериализуемости: расписание является конфликтно-сериализуемым тогда и только тогда, когда графе конфликтов для данного расписания ацикличен.

Предположим, расписание является конфликтно-сериализуеммым, но в графе есть циклов. Тогда серией свопов его можно привести к последовательному виду. Любой своп некофликтующих операций не меняет граф конфликтов, поэтому дуги не появляются и не исчезают. А в последовательном виде циклов точно нет.

Предположим, в графе конфликтов нет циклов. Тогда есть хотя бы одна транзакция с нулевой входящей степенью. Выберем одну из таких транзакций. Посмотрим на любую операцию в этой транзакции. Эта операций необязательно идет первой. Но мы знаем, что никакое из более ранних обращений не конфликтует с данной, потому что в противном случае было бы ребро, то есть входящая степень была бы положительной. Значит, эту операцию можно передвинуть к началу. А дальше повторяем.

Имея такой простой критерий, можно сделать поддерживать граф и просто отменять транзакцию, когда в графе конфликтов возникает цикл.

Но можно сделать лучше: построим алгоритм, который циклов не порождает.

### (Strict) Two-Phase Locking

Начинаем транзакцию. Отправляем планировщику чтение либо запись. Он смотрит на наше чтение либо на нашу запись по некоторому ключу.

Если он видит, что мы к этому ключу раньше не обращались, то планировщик на этот ключ берет блокировку. А дальше читаем либо пишем.

Дальше делаем следующее чтение или запись, и также, если это новый ключ, берем блокировку на него.

Локи отпускаются только после коммита или аборта.

Фаза, на которой мы берем локи, называется Expanding, а фаза, на которой отпускаем локи, называется Shrinking.

Таким образом, локов после анлоков не бывает, первый анлок происходит после последнего лока.

Докажем, что граф конфликтов для всех получаемых расписаний не содержит циклов.

Предположим, что в графе все же возник цикл: _T1_, _T2_, ..., _T1_.

Так как есть дуга из _T1_ в _T2_, то есть два конфликтующих обращения _O1'_ и _O2_, причем _O1'_ предшествует _O2_. Дуга из _T2_ в _T3_ означает, что есть два обращения _O2'_ и _O3_, причем _O2'_ предшествует _O3_. И так далее.

Из постороения алгоритма следует, что _Unlock(O1')_ предшествует _Lock(O2)_. Далее, _Unlock(O2')_ предшествует _Lock(O3)_. Но из-за двухфазности блокировок также _Lock(O2)_ предшествует _Unlock(O2')_.

Наконец, достроим теперь эту цепочку до _Lock(O1)_. Получили, что _Unlock(O1')_ предществует _Lock(O1)_. Алгоритм так сделать не мог.

Таким образом, наш алгоритм генерирует только конфликтно-сериализуемые расписания.

### Решение проблемы с дедлоками

Планировщик не знает, в каком порядке к нему приходят операции. Более того, в самой транзакции может быть условная логика, поэтому нельзя предугадать, в каком порядке будут браться локи.

Поэтому алгоритм 2PL допускает дедлоки.

Решение в лоб — поддерживать явно wait-for graph, но это затратно.

Рассмотрим более изящное решение. Допустим, две транзакции претендуют на один лок. Кто из них его больше заслуживает?

Присвоим каждой транзакции на старте временную метку. Их будет выдавать планировщик, причем монотонно. То есть чем раньше транзакция пришла, тем меньше ее метка.

Итак, если мы старая транзакция, которая видит, что молодая транзакция держит лок, который нам нужен, то можно поабортить молодую транзакцию. Если мы видим, что еще более старая транзакция держит этот лок, то уступим ее и подождем (или поабортимся сами).

После падения мы начинаем транзакцию заново, однако со старой временной меткой. В противном случае мы можем все время абортится, так как наша метка становится все больше и больше.

Здесь можно также брать на чтения _ReadLock_, а на записи _WriteLock_.

---

### Snapshot Isolation

Хотим сделать быстрее: транзакции, которые читают, не будут блокировать транзакции, которые пишут. Это значит, что по одному и тому же ключу могут быть несколько значений. Для этого нам нужна мультиверсионность.

Модифицируем наше хранилище. Теперь оно будет фиксировать не только текущее состоянии, но и всю историю изменения. Каждая версия адресуется временной меткой (ревизией). Эти временные метки монотонно растут.

Это похоже на систему контроля версий. Версии иммутабельны.

Транзакция в момент старта получает себе некоторую версию (read timestamp): из этой версии она будет читать. Продолжая аналогию, в системе контроля версий началась новая ветка.

Читает она из версии по read timestamp, а пишет — в себя.

В какой-то момент мы решаем, что пора закоммитить, и пора породить новую версию. Когда мы коммитим, мы выбираем новую версию базы и выбираем для нее новую врмеменную метку (commit timestamp). В эту версию попадают все записи, которые мы сделали.

Как и в системах контроля версий, здесь возможны конфликты. Они разрешаются по правилу _first committer wins_: если с момента старта чтения транзакции в базу были вмержены записи, которые конфликтовали с нашими записями, то наш коммит не удается.

Это правило проверяем для каждой записи в нашей транзакции. Если обнаруживается конфликт хотя бы на одном ключу, транзакция прерывается.

### Write Skew

Оказывается, засчет потери блокировок мы потеряли сериализуемость!

Допустим, приходят две транзакции, они читаются из одного снепшота и потом коммитятся. Внезапно у них это получается, поскольку конфликта между записями не возникает.

| _T1_               | _T2_               |
| ------------------ | ------------------ |
| if _Read(x) = 1_   | if _Read(y) = 1_   |
| then _Write(y, 0)_ | then _Write(x, 0)_ |

Однако в это расписание не является сериализуемым.

То, что получилось, называют аномалией (anomaly). В данном случае write skew.

Это возникает потому, что мы отлавливаем конфликты только между записями, а между чтениями и записями нет.

### DataStore

Для реализации берем Ordered Map. Операции:
* _Write(k, v, ts)_ — вставка работает так: помещаем в хранилище ключ _(k, ts)_. По этому ключу и будет упорядочено хранилище;
* _Read(k, ts)_ — читать значение ключа из максимальным таймстемпом, меньшим, чем _ts_.

Также заметим, что таймстемпы должны быть уникальными.

Благодаря этому мы никогда не перезаписываем значения.

Удалять версии можно, когда версия подрастет, и мы точно не будем из более старых читать. Это просто очистка мусора.

При этом храним мы, конечно, не версии целиком, а инкрементпальные изменения между ними.

### Two-Phase Commit

Как реализовать коммит? Навесить один мьютекс непрактично. Хотелось бы, чтобы транзакции, которы пишут по разным ключам исполнялись параллельно.

Мы не можем сразу сделать много проверок и все положить в базу. Поэтому разобьем коммит на две фазы.

Хотим независимо друг от друга сделать проверки для разных ключей, по которым пишем. На первой стадии коммита будем по очереди брать локи для каждого из ключей. После взятия лока на каждый следующий ключ проверяем: если по нему была запись после нашего read timestamp, транзакция абортится.

Если лок взять не удалось, то значит, есть конкурирующий коммит, который успел взять лок раньше. В этом случае мы тоже абортимся.

И так мы делаем для каждого ключа. Локи пока не отпускаем.

Таким образом, когда мы взяли все локи, то мы знаем, что результаты нашей проверки не устарели (так как проверка была после локов, а локи не отпускались).

Это была первая фаза, ее называют Prepare. Вторая фаза — это, собственно, Writes. Мы последовательно делаем запись. Так как у нас взяты локи, то никто нам не помешает.

### Commit Timestamp

Однако для совершения записи нужно указать таймстемп. Когда получать commit timestamp?

Если взять его в неправильный момент, алгоритм развалится. Например, если взять его до первой фазе коммита:

| _T1_         | _T2_             |
| ------------ | ---------------- |
|              | _cts = 3_        |
| _rts = 4_    |                  |
| _Read(x, 4)_ |                  |
|              | _Write(x, *, 3)_ |
|              | _Write(y, *, 3)_ |
| _Read(y, 4)_ |                  |

Здесь явно транзакции не изолированы, так как в нее по пути протекают какие-то записи (первая транзакция прочитала старый _x_ и новый _y_).

Пусть тогда чтение делает так: если оно видит, что на ключ лок взят, то нужно подождать, пока он не будет отпущен.

Однако это не решает проблему, пример никак не меняется. Чтобы это исправить, commit timestamp надо брать после взятия всех локов, то есть между Prepare и Writes. Тогда в примере пераое чтение дождется совершения всех записей.

Из монотонности таймстемпов следует, что если транзакция при чтении не увидела лок будущей транзакции, то будущая транзакция гарантированно не запишет в наш снепшот.

Действительно, _T1:rts < T1:Read(x) < T2:Lock(x) < T2:cts_.

С другой стороны, если мы попали на лок, то мы дождемся его отпускания. Но так как мы отпускаем все локи сразу, то мы увидим сразу все записи транзакции.

Таким образом, транзакции полностью изолированы, а наши снепшоты — действительно снепшоты.

Как реализовать локи? Не будем же мы использовать TaS SpinLock.

А где эти локи хранить? У нас же есть хранилище. Давайте брать лок простой записью в хранилище специального значения с особой временной меткой. Чтение этого специального значения будет эквивалентно наличию локов.
